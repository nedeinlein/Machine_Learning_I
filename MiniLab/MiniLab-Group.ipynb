{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nathan Deinlein <br>\n",
    "Ryan Kinney <br>\n",
    "Chris Roche <br>\n",
    "Cameron Stewart <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - MiniLab LR and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical technique for categorizing a binary outcome. For the horse racing data set, we're using predictors such as horse age, rating, and odds to predict whether a horse will win or lose a race.\n",
    "We began our logistic regression model implementation by importing the necessary libraries and then reading in the data. In Lab 1, once our data was cleaned we saved off a csv file titled \"runs_clean.csv\" to our GitHub repository. For this lab we read in that file as a starting point since it is already cleaned, all missing data is imputed, and features are formatted appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the cleaned dataframe from Lab 1\n",
    "url = \"https://raw.githubusercontent.com/nedeinlein/Machine_Learning_I/main/runs_clean.csv\"\n",
    "runs_df = pd.read_csv(url, index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression requires all variables to be numeric. Using the info function we can inspect the data set for objects and determine how to use or not use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79423 entries, 0 to 79422\n",
      "Data columns (total 29 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       79423 non-null  int64  \n",
      " 1   race_id          79423 non-null  int64  \n",
      " 2   horse_no         79423 non-null  int64  \n",
      " 3   horse_id         79423 non-null  int64  \n",
      " 4   result           79423 non-null  int64  \n",
      " 5   won              79423 non-null  float64\n",
      " 6   lengths_behind   79423 non-null  float64\n",
      " 7   horse_age        79423 non-null  int64  \n",
      " 8   horse_country    79423 non-null  object \n",
      " 9   horse_type       79423 non-null  object \n",
      " 10  horse_rating     79423 non-null  int64  \n",
      " 11  horse_gear       79423 non-null  object \n",
      " 12  declared_weight  79423 non-null  float64\n",
      " 13  actual_weight    79423 non-null  int64  \n",
      " 14  draw             79423 non-null  int64  \n",
      " 15  position_sec1    79423 non-null  int64  \n",
      " 16  position_sec2    79423 non-null  int64  \n",
      " 17  position_sec3    79423 non-null  int64  \n",
      " 18  behind_sec1      79423 non-null  float64\n",
      " 19  behind_sec2      79423 non-null  float64\n",
      " 20  behind_sec3      79423 non-null  float64\n",
      " 21  time1            79423 non-null  float64\n",
      " 22  time2            79423 non-null  float64\n",
      " 23  time3            79423 non-null  float64\n",
      " 24  finish_time      79423 non-null  float64\n",
      " 25  win_odds         79423 non-null  float64\n",
      " 26  place_odds       79423 non-null  float64\n",
      " 27  trainer_id       79423 non-null  int64  \n",
      " 28  jockey_id        79423 non-null  int64  \n",
      "dtypes: float64(12), int64(14), object(3)\n",
      "memory usage: 17.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>race_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>result</th>\n",
       "      <th>won</th>\n",
       "      <th>lengths_behind</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_country</th>\n",
       "      <th>horse_type</th>\n",
       "      <th>...</th>\n",
       "      <th>behind_sec2</th>\n",
       "      <th>behind_sec3</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>jockey_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3917</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>13.85</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.86</td>\n",
       "      <td>83.92</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2157</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>9.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.57</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.30</td>\n",
       "      <td>83.56</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.69</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.90</td>\n",
       "      <td>83.40</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3</td>\n",
       "      <td>SAF</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>14.09</td>\n",
       "      <td>21.83</td>\n",
       "      <td>23.70</td>\n",
       "      <td>83.62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>80</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2796</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3</td>\n",
       "      <td>GB</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14.77</td>\n",
       "      <td>21.75</td>\n",
       "      <td>23.22</td>\n",
       "      <td>83.24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  race_id  horse_no  horse_id  result  won  lengths_behind  \\\n",
       "0           0        0         1      3917      10  0.0            8.00   \n",
       "1           1        0         2      2157       8  0.0            5.75   \n",
       "2           2        0         3       858       7  0.0            4.75   \n",
       "3           3        0         4      1853       9  0.0            6.25   \n",
       "4           4        0         5      2796       6  0.0            3.75   \n",
       "\n",
       "   horse_age horse_country horse_type  ...  behind_sec2 behind_sec3  time1  \\\n",
       "0          3           AUS    Gelding  ...         2.00        1.50  13.85   \n",
       "1          3            NZ    Gelding  ...         9.00        5.00  14.57   \n",
       "2          3            NZ    Gelding  ...         1.00        0.75  13.69   \n",
       "3          3           SAF    Gelding  ...         5.00        3.50  14.09   \n",
       "4          3            GB    Gelding  ...         8.75        4.25  14.77   \n",
       "\n",
       "   time2  time3  finish_time  win_odds  place_odds  trainer_id  jockey_id  \n",
       "0  21.59  23.86        83.92       9.7         3.7         118          2  \n",
       "1  21.99  23.30        83.56      16.0         4.9         164         57  \n",
       "2  21.59  23.90        83.40       3.5         1.5         137         18  \n",
       "3  21.83  23.70        83.62      39.0        11.0          80         59  \n",
       "4  21.75  23.22        83.24      50.0        14.0           9        154  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics of each feature\n",
    "runs_df.info()\n",
    "runs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horse Country and Horse Type are both of type object. These are strings that represent where a horse is from or what type it is (e.g. gelding, mare). For both of these, we used one-hot encoding. This allows us to numerically represent them and therefore keep them in the logistic regression model.\n",
    "\n",
    "First we'll check how many unique types there are in the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of horse types:  9\n",
      "Number of horse countries:  16\n"
     ]
    }
   ],
   "source": [
    "# How many unique values are in horse_type and horse_country\n",
    "n = runs_df['horse_type'].nunique()\n",
    "print(\"Number of horse types: \", n)\n",
    "\n",
    "n = runs_df['horse_country'].nunique()\n",
    "print(\"Number of horse countries: \", n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will perform the one hot encoding. Note, this will greatly increase the number of columns in our data set since we're now making boolean entries for each of the nine horse types and sixteen countries found in the data set. Therefore, we'll add the new columns to a second data set so that we can easily choose whether or not to include the new features in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one hot encoding on non-numerical features\n",
    "## (Then remove them from the drop code chunk below)\n",
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "tmp_df = pd.get_dummies(runs_df.horse_country,prefix='horse_country')\n",
    "runs_df_onehot = pd.concat((runs_df,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "tmp_df = pd.get_dummies(runs_df.horse_type,prefix='horse_type')\n",
    "runs_df_onehot = pd.concat((runs_df_onehot,tmp_df),axis=1) # add back into the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the one-hot encoding, we want to add another new feature to the set, this new feature is something we'll try to predict. There are two common types of bets in horse racing: to win and to show. Win is for a horse to finish first and show is for a horse to finish top three. Using the result feature (a horse's finish position from 1 - 14) we can create a new feature for a result of 1, 2, or 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new Show feature to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>race_id</th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>result</th>\n",
       "      <th>won</th>\n",
       "      <th>lengths_behind</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_country</th>\n",
       "      <th>horse_type</th>\n",
       "      <th>...</th>\n",
       "      <th>behind_sec3</th>\n",
       "      <th>time1</th>\n",
       "      <th>time2</th>\n",
       "      <th>time3</th>\n",
       "      <th>finish_time</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>jockey_id</th>\n",
       "      <th>show</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3917</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>13.85</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.86</td>\n",
       "      <td>83.92</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2157</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>14.57</td>\n",
       "      <td>21.99</td>\n",
       "      <td>23.30</td>\n",
       "      <td>83.56</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>858</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.69</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.90</td>\n",
       "      <td>83.40</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>3</td>\n",
       "      <td>SAF</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>14.09</td>\n",
       "      <td>21.83</td>\n",
       "      <td>23.70</td>\n",
       "      <td>83.62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>80</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2796</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3</td>\n",
       "      <td>GB</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14.77</td>\n",
       "      <td>21.75</td>\n",
       "      <td>23.22</td>\n",
       "      <td>83.24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3296</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>14.33</td>\n",
       "      <td>22.03</td>\n",
       "      <td>22.90</td>\n",
       "      <td>82.83</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>54</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>911</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13.53</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.94</td>\n",
       "      <td>84.15</td>\n",
       "      <td>99.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>55</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2170</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>14.13</td>\n",
       "      <td>21.87</td>\n",
       "      <td>23.58</td>\n",
       "      <td>82.64</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>47</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1730</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.75</td>\n",
       "      <td>3</td>\n",
       "      <td>NZ</td>\n",
       "      <td>Gelding</td>\n",
       "      <td>...</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13.65</td>\n",
       "      <td>21.71</td>\n",
       "      <td>23.90</td>\n",
       "      <td>84.20</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>75</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2998</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.00</td>\n",
       "      <td>3</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Mare</td>\n",
       "      <td>...</td>\n",
       "      <td>16.75</td>\n",
       "      <td>15.05</td>\n",
       "      <td>22.31</td>\n",
       "      <td>24.38</td>\n",
       "      <td>92.20</td>\n",
       "      <td>39.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>109</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  race_id  horse_no  horse_id  result  won  lengths_behind  \\\n",
       "0           0        0         1      3917      10  0.0            8.00   \n",
       "1           1        0         2      2157       8  0.0            5.75   \n",
       "2           2        0         3       858       7  0.0            4.75   \n",
       "3           3        0         4      1853       9  0.0            6.25   \n",
       "4           4        0         5      2796       6  0.0            3.75   \n",
       "5           5        0         6      3296       3  0.0            1.25   \n",
       "6           6        0         7       911      12  0.0            9.50   \n",
       "7           7        0         8      2170       1  1.0            0.00   \n",
       "8           8        0         9      1730      13  0.0            9.75   \n",
       "9           9        0        10      2998      14  0.0          999.00   \n",
       "\n",
       "   horse_age horse_country horse_type  ...  behind_sec3  time1  time2  time3  \\\n",
       "0          3           AUS    Gelding  ...         1.50  13.85  21.59  23.86   \n",
       "1          3            NZ    Gelding  ...         5.00  14.57  21.99  23.30   \n",
       "2          3            NZ    Gelding  ...         0.75  13.69  21.59  23.90   \n",
       "3          3           SAF    Gelding  ...         3.50  14.09  21.83  23.70   \n",
       "4          3            GB    Gelding  ...         4.25  14.77  21.75  23.22   \n",
       "5          3            NZ    Gelding  ...         1.25  14.33  22.03  22.90   \n",
       "6          3            NZ    Gelding  ...         0.75  13.53  21.59  23.94   \n",
       "7          3           AUS    Gelding  ...         3.25  14.13  21.87  23.58   \n",
       "8          3            NZ    Gelding  ...         1.25  13.65  21.71  23.90   \n",
       "9          3           AUS       Mare  ...        16.75  15.05  22.31  24.38   \n",
       "\n",
       "   finish_time  win_odds  place_odds  trainer_id  jockey_id  show  \n",
       "0        83.92       9.7         3.7         118          2     0  \n",
       "1        83.56      16.0         4.9         164         57     0  \n",
       "2        83.40       3.5         1.5         137         18     0  \n",
       "3        83.62      39.0        11.0          80         59     0  \n",
       "4        83.24      50.0        14.0           9        154     0  \n",
       "5        82.83       7.0         1.8          54         34     1  \n",
       "6        84.15      99.0        28.0          55        149     0  \n",
       "7        82.64      12.0         3.6          47        183     1  \n",
       "8        84.20      38.0        13.0          75        131     0  \n",
       "9        92.20      39.0        12.0         109        145     0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a Show result feature\n",
    "runs_df['show'] = np.where(runs_df['result'] <= 3, 1, 0)\n",
    "runs_df_onehot['show'] = np.where(runs_df['result'] <= 3, 1, 0)\n",
    "\n",
    "runs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the necessary features and the data set is cleaned, we will divide into test and train data sets. We used an 80/20 split: 80% of the data will be used to train the models and the remaining 20% will be used to generate a confusion matrix so that we can evaluate performance and compare various models.\n",
    "\n",
    "Since the data set contains features not known pre-race (e.g. how many lengths the horse finished behind the winner, lap times, etc.) we will drop those from the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and Train set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of three test/train data sets were created. One for runs_data without one hot encoding, one for runs_data with one-hot encoding, and one with one-hot encoding but the target is \"show\" instead of \"win\". All are derived from the same data set with the same random seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and target\n",
    "# Only use features known pre-race (e.g. not finish time)\n",
    "# Remove non-numerical features (e.g. horse country) since we performed one hot encoding\n",
    "runs_data = runs_df.drop(['Unnamed: 0','race_id','horse_id','result','won','show','lengths_behind','horse_country','horse_type','horse_gear','position_sec1','position_sec2','position_sec3','behind_sec1','behind_sec2','behind_sec3','time1','time2','time3','finish_time','trainer_id','jockey_id'], axis=1)\n",
    "runs_target = runs_df['won']\n",
    "runs_target_show = runs_df['show']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and target\n",
    "# Only use features known pre-race (e.g. not finish time)\n",
    "# Remove non-numerical features (e.g. horse country) since we performed one hot encoding\n",
    "runs_data_onehot = runs_df_onehot.drop(['Unnamed: 0','race_id','horse_id','result','won','show','lengths_behind','horse_country','horse_type','horse_gear','position_sec1','position_sec2','position_sec3','behind_sec1','behind_sec2','behind_sec3','time1','time2','time3','finish_time','trainer_id','jockey_id'], axis=1)\n",
    "runs_target_onehot = runs_df_onehot['won']\n",
    "runs_target_show_onehot = runs_df_onehot['show']\n",
    "\n",
    "X_train_onehot,X_test_onehot,Y_train_onehot,Y_test_onehot = train_test_split(runs_data_onehot,runs_target_onehot,test_size=0.20,random_state=0)\n",
    "X_train_onehot_show,X_test_onehot_show,Y_train_onehot_show,Y_test_onehot_show = train_test_split(runs_data_onehot,runs_target_show_onehot,test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we created two logistic regression models to predict whether a horse won or lost its race. For the first model we do not balance the class weights. For the second we do. Note the dramatically higher accuracy for unbalanced classes. For an unbalanced model, it simply predicts a loss for every outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9207428391564368\n",
      "AUC:  0.5\n",
      "Confusion Matrix: \n",
      " [[14626     0]\n",
      " [ 1259     0]]\n"
     ]
    }
   ],
   "source": [
    "# Use Logistic Regression to predict win/loss - Not Balance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_nobalance = LogisticRegression(penalty='l2', C=1.0, solver='liblinear' )\n",
    "lr_nobalance.fit(X_train_runs,Y_train_runs)\n",
    "yhat = lr_nobalance.predict(X_test_runs)\n",
    "\n",
    "acc = mt.accuracy_score(Y_test_runs,yhat)\n",
    "auc = roc_auc_score(Y_test_runs,yhat)\n",
    "conf = mt.confusion_matrix(Y_test_runs,yhat)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"AUC: \", auc )\n",
    "print(\"Confusion Matrix: \\n\",conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9207428391564368\n",
      "AUC:  0.5\n",
      "Confusion Matrix: \n",
      " [[14626     0]\n",
      " [ 1259     0]]\n"
     ]
    }
   ],
   "source": [
    "# Use Logistic Regression to predict win/loss - Class Balance\n",
    "lr_balance = LogisticRegression(penalty='l2', C=1.0, class_weight='balanced', solver='liblinear')\n",
    "lr_nobalance.fit(X_train_runs,Y_train_runs)\n",
    "yhat = lr_nobalance.predict(X_test_runs)\n",
    "\n",
    "acc = mt.accuracy_score(Y_test_runs,yhat)\n",
    "auc = roc_auc_score(Y_test_runs,yhat)\n",
    "conf = mt.confusion_matrix(Y_test_runs,yhat)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"AUC: \", auc )\n",
    "print(\"Confusion Matrix: \\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning: Class Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen below, of the 79423 entries in the data set, only 6360 (8%) won their race. This intuitively makes sense since a horse can finish from first to about fourteenth place depending on the race. When we did not balance the class the logistic regression model selected a loss for every prediction and achieved an accuracy of about 92%, which again intuitively makes sense if 92% of entries are loses.\n",
    "\n",
    "When we account for this imbalance, our logistic regression model is able to correctly predict whether a horse will win or lose 55.5% of the time. While less accurate than the unbalance model, it is more useful in a real world scenario where we want to make actual picks on which horse will win a race and not just pick every horse to lose all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    73063\n",
      "1.0     6360\n",
      "Name: won, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# How many won, lost values in the data set\n",
    "item_counts = runs_df['won'].value_counts()\n",
    "print(item_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plotted the confusion matrix for the balanced logistic regression model to help visually interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1ef84be7b761>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot the confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_balance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"plot_confusion_matrix only supports classifiers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m     cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,\n\u001b[0;32m    221\u001b[0m                           labels=labels, normalize=normalize)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mclass\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(lr_balance, X_test_onehot, Y_test_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using One Hot Encoding to create a third model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a logistic regression model using the one hot encoded features and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5563109852061694\n",
      "AUC:  0.694090284126313\n",
      "Confusion Matrix: \n",
      " [[7757 6869]\n",
      " [ 179 1080]]\n"
     ]
    }
   ],
   "source": [
    "lr_onehot = LogisticRegression(penalty='l2', C=1.0, class_weight='balanced', solver='liblinear' )\n",
    "lr_onehot.fit(X_train_onehot,Y_train_onehot)\n",
    "yhat = lr_onehot.predict(X_test_onehot)\n",
    "\n",
    "acc = mt.accuracy_score(Y_test_onehot,yhat)\n",
    "auc = roc_auc_score(Y_test_onehot,yhat)\n",
    "conf = mt.confusion_matrix(Y_test_onehot,yhat)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Confusion Matrix: \\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the additional features based on horse type and horse country, we increased our performance to 55.6248% at the cost of some added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the three logistic regression models to predict a win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the three models so far:\n",
    "- Logistic Regression with basic features from the data set: 69.4756% AUC\n",
    "- Logistic Regression with only odds predictors: 69.6173% AUC\n",
    "- Logistic Regression with additional one-hot encoded features: 69.4056% AUC\n",
    "\n",
    "Note the Logistic Regression model with only the odds predictors has the highest AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model to predict if a horse will show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we quickly created a logistic regression model using balanced classes and the horse country and type features, but predicted \"show\" instead of \"win\". So, this model predicts whether a horse will finish first, second, or third. Note, it achieved an accuracy of 59.3642% and an AUC of 67.7663%. The model is about 4% more accurate that predicting wins/losses but it has a lower AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5938306578533208\n",
      "AUC:  0.6786284111976159\n",
      "Confusion Matrix: \n",
      " [[6280 5917]\n",
      " [ 535 3153]]\n"
     ]
    }
   ],
   "source": [
    "lr_show = LogisticRegression(penalty='l2', C=1.0, class_weight='balanced', solver='liblinear' )\n",
    "lr_show.fit(X_train_onehot_show,Y_train_onehot_show)\n",
    "yhat = lr_show.predict(X_test_onehot_show)\n",
    "\n",
    "acc = mt.accuracy_score(Y_test_onehot_show,yhat)\n",
    "auc = roc_auc_score(Y_test_onehot,yhat)\n",
    "conf = mt.confusion_matrix(Y_test_onehot_show,yhat)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"AUC: \", auc )\n",
    "print(\"Confusion Matrix: \\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Support Vector Machine (SVM) requires the data to be pre-processed into the appropriate format before the model can be trained. SVMs can handle both numerical and categorical predictors to classify a response. We are using this SVM to predict the won variable which describes whether the horse wone the race. To ensure the range of a predictors does not contribute to the influence of the variable over the response, we will scale the data first. Next, the data needs to be one-hot encoded. Attributes with many categories can cause the model training to be greatly slowed, so we will limit the categorical levels before encoding. With the data frame in the proper format, we need to balance the dataset to ensure the minority class is not ignored. We used down sampling of the losing horse class to balance the number of winning and losing horses. Finally, we will tune our model parameters and predict our test set. We used an 80/20 test/train split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/nedeinlein/Machine_Learning_I/main/runs_clean.csv\"\n",
    "runs_df = pd.read_csv(url, index_col=False)\n",
    "runs_data = runs_df.drop(['Unnamed: 0','race_id','result','won','lengths_behind',\n",
    "                          #'horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id'\n",
    "                          'position_sec1','position_sec2',\n",
    "                          'position_sec3','behind_sec1','behind_sec2','behind_sec3','time1','time2',\n",
    "                          'time3','finish_time'], axis=1)\n",
    "runs_target = runs_df['won']\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(runs_data,runs_target,test_size=0.20,random_state=0)# Add a Place result feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horse_no</th>\n",
       "      <th>horse_age</th>\n",
       "      <th>horse_rating</th>\n",
       "      <th>declared_weight</th>\n",
       "      <th>actual_weight</th>\n",
       "      <th>draw</th>\n",
       "      <th>win_odds</th>\n",
       "      <th>place_odds</th>\n",
       "      <th>horse_id</th>\n",
       "      <th>trainer_id</th>\n",
       "      <th>jockey_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>6.353800e+04</td>\n",
       "      <td>63538.000000</td>\n",
       "      <td>63538.000000</td>\n",
       "      <td>63538.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.725846e-18</td>\n",
       "      <td>6.377431e-17</td>\n",
       "      <td>2.029043e-16</td>\n",
       "      <td>-1.087930e-15</td>\n",
       "      <td>7.200289e-16</td>\n",
       "      <td>1.481742e-17</td>\n",
       "      <td>1.276115e-16</td>\n",
       "      <td>1.287298e-16</td>\n",
       "      <td>2203.900249</td>\n",
       "      <td>79.829016</td>\n",
       "      <td>85.929585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1.000008e+00</td>\n",
       "      <td>1275.321374</td>\n",
       "      <td>45.123485</td>\n",
       "      <td>54.310712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.568999e+00</td>\n",
       "      <td>-1.527638e+00</td>\n",
       "      <td>-4.343842e+00</td>\n",
       "      <td>-6.608647e+00</td>\n",
       "      <td>-3.133311e+00</td>\n",
       "      <td>-1.567262e+00</td>\n",
       "      <td>-9.238534e-01</td>\n",
       "      <td>-7.433377e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.707318e-01</td>\n",
       "      <td>-3.863104e-01</td>\n",
       "      <td>-8.556892e-02</td>\n",
       "      <td>-6.881142e-01</td>\n",
       "      <td>-7.514050e-01</td>\n",
       "      <td>-7.662460e-01</td>\n",
       "      <td>-7.011894e-01</td>\n",
       "      <td>-5.815941e-01</td>\n",
       "      <td>1085.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.753526e-02</td>\n",
       "      <td>-3.863104e-01</td>\n",
       "      <td>-8.556892e-02</td>\n",
       "      <td>-4.632208e-02</td>\n",
       "      <td>4.256376e-02</td>\n",
       "      <td>3.476977e-02</td>\n",
       "      <td>-4.585852e-01</td>\n",
       "      <td>-3.505319e-01</td>\n",
       "      <td>2209.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.258024e-01</td>\n",
       "      <td>-3.863104e-01</td>\n",
       "      <td>-8.556892e-02</td>\n",
       "      <td>6.596493e-01</td>\n",
       "      <td>8.365325e-01</td>\n",
       "      <td>8.357855e-01</td>\n",
       "      <td>3.057839e-01</td>\n",
       "      <td>8.848643e-02</td>\n",
       "      <td>3307.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>138.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.890159e+00</td>\n",
       "      <td>7.602982e+00</td>\n",
       "      <td>6.557338e+00</td>\n",
       "      <td>4.237641e+00</td>\n",
       "      <td>1.630501e+00</td>\n",
       "      <td>1.903806e+00</td>\n",
       "      <td>2.333024e+00</td>\n",
       "      <td>1.080978e+01</td>\n",
       "      <td>4404.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           horse_no     horse_age  horse_rating  declared_weight  \\\n",
       "count  6.353800e+04  6.353800e+04  6.353800e+04     6.353800e+04   \n",
       "mean  -2.725846e-18  6.377431e-17  2.029043e-16    -1.087930e-15   \n",
       "std    1.000008e+00  1.000008e+00  1.000008e+00     1.000008e+00   \n",
       "min   -1.568999e+00 -1.527638e+00 -4.343842e+00    -6.608647e+00   \n",
       "25%   -7.707318e-01 -3.863104e-01 -8.556892e-02    -6.881142e-01   \n",
       "50%    2.753526e-02 -3.863104e-01 -8.556892e-02    -4.632208e-02   \n",
       "75%    8.258024e-01 -3.863104e-01 -8.556892e-02     6.596493e-01   \n",
       "max    1.890159e+00  7.602982e+00  6.557338e+00     4.237641e+00   \n",
       "\n",
       "       actual_weight          draw      win_odds    place_odds      horse_id  \\\n",
       "count   6.353800e+04  6.353800e+04  6.353800e+04  6.353800e+04  63538.000000   \n",
       "mean    7.200289e-16  1.481742e-17  1.276115e-16  1.287298e-16   2203.900249   \n",
       "std     1.000008e+00  1.000008e+00  1.000008e+00  1.000008e+00   1275.321374   \n",
       "min    -3.133311e+00 -1.567262e+00 -9.238534e-01 -7.433377e-01      0.000000   \n",
       "25%    -7.514050e-01 -7.662460e-01 -7.011894e-01 -5.815941e-01   1085.000000   \n",
       "50%     4.256376e-02  3.476977e-02 -4.585852e-01 -3.505319e-01   2209.000000   \n",
       "75%     8.365325e-01  8.357855e-01  3.057839e-01  8.848643e-02   3307.000000   \n",
       "max     1.630501e+00  1.903806e+00  2.333024e+00  1.080978e+01   4404.000000   \n",
       "\n",
       "         trainer_id     jockey_id  \n",
       "count  63538.000000  63538.000000  \n",
       "mean      79.829016     85.929585  \n",
       "std       45.123485     54.310712  \n",
       "min        0.000000      0.000000  \n",
       "25%       47.000000     39.000000  \n",
       "50%       75.000000     76.000000  \n",
       "75%      118.000000    138.000000  \n",
       "max      175.000000    185.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Normalize the scales of variables (no need to scale y as that is the prediction)\n",
    "\n",
    "#Remove categorical vars\n",
    "cat_x_train = X_train[['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id']]\n",
    "cat_x_test = X_test[['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id']]\n",
    "num_x_train = X_train.drop(['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id'],axis=1)\n",
    "num_x_test = X_test.drop(['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id'],axis=1)\n",
    "\n",
    "#Very important to fit the scale to only the training data set... \n",
    "#and apply the transfromation to the test data set\n",
    "scl_obj = StandardScaler()\n",
    "scl_x_train = scl_obj.fit_transform(num_x_train)\n",
    "scl_x_test = scl_obj.transform(num_x_test)\n",
    "\n",
    "scl_x_train=pd.DataFrame(scl_x_train, columns=num_x_train.columns)\n",
    "scl_x_test=pd.DataFrame(scl_x_test, columns=num_x_test.columns)\n",
    "\n",
    "#Add categorical variables back in\n",
    "scl_x_train.index = cat_x_train.index\n",
    "scl_x_test.index = cat_x_test.index\n",
    "scl_x_train = pd.concat([scl_x_train, cat_x_train],axis=1)\n",
    "scl_x_test = pd.concat([scl_x_test, cat_x_test],axis=1)\n",
    "\n",
    "scl_x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data allows us to keep any one category from unnecessarily affecting the outcome due to the range of its data. This is done by using the standard scaler function. This maps all continuous variables to a common scale while keeping the deviation from mean standard. This does not affect the data as the deviation remains constant and only the range is changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limiting the Categories\n",
    "\n",
    "This is done to keep the number of one hot encoded categories we are about to create from getting out of hand and causing the model to be very resource intensive to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horse ID\n",
      "original:  4308\n",
      "new:  12\n",
      "['Other' 107 2382 1849 4314 3887 779 4058 4041 493 3323 70] \n",
      "\n",
      "Horse Country\n",
      "original:  16\n",
      "new:  10\n",
      "['NZ' 'USA' 'AUS' 'GB' 'IRE' 'FR' 'SAF' 'GER' 'Other' 'ARG'] \n",
      "\n",
      "Horse Type\n",
      "original:  9\n",
      "new:  7\n",
      "['Gelding' 'Brown' 'Other' 'Colt' 'Horse' 'Mare' 'Rig'] \n",
      "\n",
      "Horse Gear\n",
      "original:  735\n",
      "new:  10\n",
      "['--' 'TT/B' 'Other' 'B' 'CP' 'TT/H' 'H' 'TT' 'XB' 'SR'] \n",
      "\n",
      "Trainer ID\n",
      "original:  155\n",
      "new:  13\n",
      "[97 118 7 80 69 'Other' 137 98 75 47 55 164 138] \n",
      "\n",
      "Jockey ID\n",
      "original:  178\n",
      "new:  13\n",
      "[76 175 'Other' 162 50 40 34 63 2 138 64 18 149]\n"
     ]
    }
   ],
   "source": [
    "cat_lim_train=scl_x_train.copy()\n",
    "cat_lim_test=scl_x_test.copy()\n",
    "\n",
    "#Update Horse ID to horses who have raced over 60 times\n",
    "horse_id_counts = cat_lim_train.horse_id.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.horse_id.isin(horse_id_counts.index[horse_id_counts.gt(60)]),'horse_id']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.horse_id.isin(horse_id_counts.index[horse_id_counts.gt(60)]),'horse_id']='Other'\n",
    "print('Horse ID')\n",
    "print('original: ',len(scl_x_train.horse_id.unique()))\n",
    "print('new: ',len(cat_lim_train.horse_id.unique()))\n",
    "print(cat_lim_train.horse_id.unique(),\"\\n\")\n",
    "\n",
    "#Update Horse Country to countries that appear over 80 times\n",
    "horse_country_counts = cat_lim_train.horse_country.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.horse_country.\n",
    "                  isin(horse_country_counts.index[horse_country_counts.gt(80)]),'horse_country']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.horse_country.\n",
    "                 isin(horse_country_counts.index[horse_country_counts.gt(80)]),'horse_country']='Other'\n",
    "print('Horse Country')\n",
    "print('original: ',len(scl_x_train.horse_country.unique()))\n",
    "print('new: ',len(cat_lim_train.horse_country.unique()))\n",
    "print(cat_lim_train.horse_country.unique(),\"\\n\")\n",
    "\n",
    "#Update Horse Type to types that appear over 50 times\n",
    "horse_type_counts = cat_lim_train.horse_type.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.horse_type.isin(horse_type_counts.index[horse_type_counts.gt(50)]),\n",
    "                  'horse_type']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.horse_type.isin(horse_type_counts.index[horse_type_counts.gt(50)]),\n",
    "                 'horse_type']='Other'\n",
    "print('Horse Type')\n",
    "print('original: ',len(scl_x_train.horse_type.unique()))\n",
    "print('new: ',len(cat_lim_train.horse_type.unique()))\n",
    "print(cat_lim_train.horse_type.unique(),\"\\n\")\n",
    "\n",
    "#Update Horse Gear to gear combos that appear over 300 times\n",
    "horse_gear_counts = cat_lim_train.horse_gear.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.horse_gear.isin(horse_gear_counts.index[horse_gear_counts.gt(300)]),\n",
    "                  'horse_gear']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.horse_gear.isin(horse_gear_counts.index[horse_gear_counts.gt(300)]),\n",
    "                 'horse_gear']='Other'\n",
    "print('Horse Gear')\n",
    "print('original: ',len(scl_x_train.horse_gear.unique()))\n",
    "print('new: ',len(cat_lim_train.horse_gear.unique()))\n",
    "print(cat_lim_train.horse_gear.unique(),\"\\n\")\n",
    "\n",
    "#Update TrainerID to trainers that appear over 2500 times\n",
    "trainer_id_counts = cat_lim_train.trainer_id.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.trainer_id.isin(trainer_id_counts.index[trainer_id_counts.gt(2500)]),\n",
    "                  'trainer_id']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.trainer_id.isin(trainer_id_counts.index[trainer_id_counts.gt(2500)]),\n",
    "                 'trainer_id']='Other'\n",
    "print('Trainer ID')\n",
    "print('original: ',len(scl_x_train.trainer_id.unique()))\n",
    "print('new: ',len(cat_lim_train.trainer_id.unique()))\n",
    "print(cat_lim_train.trainer_id.unique(),\"\\n\")\n",
    "\n",
    "#Update JockeyID to jockeys that appear over 2000 times\n",
    "jockey_id_counts = cat_lim_train.jockey_id.value_counts()\n",
    "cat_lim_train.loc[~cat_lim_train.jockey_id.isin(jockey_id_counts.index[jockey_id_counts.gt(2000)]),\n",
    "                  'jockey_id']='Other'\n",
    "cat_lim_test.loc[~cat_lim_test.jockey_id.isin(jockey_id_counts.index[jockey_id_counts.gt(2000)]),\n",
    "                 'jockey_id']='Other'\n",
    "print('Jockey ID')\n",
    "print('original: ',len(scl_x_train.jockey_id.unique()))\n",
    "print('new: ',len(cat_lim_train.jockey_id.unique()))\n",
    "print(cat_lim_train.jockey_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the categorical variables in the dataset have hundreds or thousands of categories. To simplify the classes, we will use a minimum instance limit for each categorical variable to determine which classes to keep (instance limit in code notes above each section). Any class not meeting the instance limit, will be classified as 'Other'. This generalization of the smaller classes will be done using the training data set and then the results will be applied to the test data set. In the above output, each categorical variable is shown with the original number of categories and the new number of categories after generalization. A list of the new unique categories is listed at the end of each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode\n",
    "For the SVM to be able to interpret categorical data, it must be one-hot encoded. This means that each class in each categorical variable will become a column. By limiting the categorical variable classes above, we ensured the data frame will not overly expand. After creating the dummy variables and adding them to our existing test and training sets, we must drop the original categorical variables and one class from each categorical variable. The 'Other' class was dropped from each categorical variable. The below output shows the shape of the training and test sets before and after performing the one-hot encoding process. The data sets do not change the number of instances but they expanded from 14 to 67 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Set Shape Before Dummies:\n",
      " (63538, 14)\n",
      "Original Test Set Shape Before Dummies:\n",
      " (15885, 14)\n",
      "New Training Set Shape After Dummies:\n",
      " (63538, 67)\n",
      "New Test Set Shape After Dummies:\n",
      " (15885, 67)\n"
     ]
    }
   ],
   "source": [
    "#Create Dummies for training and test\n",
    "dummies_train = pd.get_dummies(\n",
    "    cat_lim_train[['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id']])\n",
    "dummies_test = pd.get_dummies(\n",
    "    cat_lim_test[['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id']])\n",
    "\n",
    "#Add dummies back to original data frames\n",
    "final_train = pd.concat([cat_lim_train, dummies_train],axis=1)\n",
    "final_test = pd.concat([cat_lim_test, dummies_test],axis=1)\n",
    "\n",
    "#Drop original variable and one class for each variable (the dropped class is 'Other')\n",
    "final_train = final_train.drop(['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id',\n",
    "                            'horse_id_Other','horse_country_Other','horse_type_Other','horse_gear_Other',\n",
    "                            'trainer_id_Other','jockey_id_Other'], axis=1)\n",
    "final_test = final_test.drop(['horse_id','horse_country','horse_type','horse_gear','trainer_id','jockey_id',\n",
    "                            'horse_id_Other','horse_country_Other','horse_type_Other','horse_gear_Other',\n",
    "                            'trainer_id_Other','jockey_id_Other'], axis=1)\n",
    "\n",
    "print('Original Training Set Shape Before Dummies:\\n',cat_lim_train.shape)\n",
    "print('Original Test Set Shape Before Dummies:\\n',cat_lim_test.shape)\n",
    "print('New Training Set Shape After Dummies:\\n',final_train.shape)\n",
    "print('New Test Set Shape After Dummies:\\n',final_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Models\n",
    "Below are the test SVM models that were run to find the optimum C value. It is important to note that because our dataset is not balanced (equal number of instances of our binary predicted category), we had to insert into the SVM code that we wanted the data balanced. This keeps the model from defaulting to a prediction of negative or positive just because it will be accurate a large percentage of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5771482530689329\n",
      "auc: 0.6999614263695484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x24a8ba326d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=1, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=5, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=10, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=15, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=20, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model just as before\n",
    "svm_clf = SVC(C=30, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(final_train, Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(final_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing PCA to the SVM.\n",
    "##### We wanted to see if PCA would allow us to create a simpler model while maintaining or beating the results of our previous SVM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "scaler = MinMaxScaler()\n",
    "pca = PCA(n_components = 0.90)\n",
    "\n",
    "pca_train = final_train[['horse_age','horse_rating','declared_weight','actual_weight','draw','win_odds','place_odds']]\n",
    "pca_train_cat = final_train.drop(['horse_age','horse_rating','declared_weight','actual_weight','draw','win_odds','place_odds'],axis = 1)\n",
    "x_pcatrain = pca.fit(pca_train).transform(pca_train)\n",
    "train_rescaled = scaler.fit_transform(x_pcatrain)\n",
    "x_pcatrain_scaled = pd.DataFrame(train_rescaled)\n",
    "\n",
    "\n",
    "pca_test = final_test[['horse_age','horse_rating','declared_weight','actual_weight','draw','win_odds','place_odds']]\n",
    "pca_test_cat = final_test.drop(['horse_age','horse_rating','declared_weight','actual_weight','draw','win_odds','place_odds'],axis = 1)\n",
    "x_pcatest = pca.fit(pca_test).transform(pca_test)\n",
    "test_rescaled = scaler.fit_transform(x_pcatest)\n",
    "x_pcatest_scaled = pd.DataFrame(x_pcatest)\n",
    "\n",
    "x_pcatrain_scaled.index=pca_train_cat.index\n",
    "x_pcatest_scaled.index=pca_test_cat.index\n",
    "x_pcatrain_scaled = pd.concat([x_pcatrain_scaled,pca_train_cat],axis = 1)\n",
    "x_pcatest_scaled = pd.concat([x_pcatest_scaled,pca_test_cat],axis = 1)\n",
    "print(x_pcatrain_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes the continuous variables and reduces their number from 7 down to 2 while maintaining 90% of the variability in the data. We then used this to run 3 models similar to the most successful above to judge efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "svm_clf = SVC(C=5, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(x_pcatrain_scaled,Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(x_pcatest_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "svm_clf = SVC(C=10, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(x_pcatrain_scaled,Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(x_pcatest_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(C=15, kernel='rbf', gamma='auto',class_weight = 'balanced') # get object\n",
    "svm_clf.fit(x_pcatrain_scaled,Y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(x_pcatest_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,y_hat)\n",
    "conf = mt.confusion_matrix(Y_test,y_hat)\n",
    "auc=roc_auc_score(Y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('auc:',auc)\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=conf,  display_labels=svm_clf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the various SVM models with PCA, it was determined that none of them could create a model as good as the one we created without PCA. As a result we chose to go with the model of C = 20, for our best SVM model to then compare against the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at these two types of models we determined that AUC should be the main judge of model performance as it looks for a middle ground between sensitivity, specificity, and accuracy. This allows us to keep a much more balanced model that seeks the middle ground of these three instead of working on simply accuracy. Since we know ~90% accuracy can be reached simply by predicting all horses will lose, which is not reasonable, we need to have the model at least attempting to determine positive results. Our best logistic regression model for predicting wins came in with an AUC of .694. This is not an ideal high but it does show that the model should be more effective than our minimum request of random chance (50/50). \n",
    "After testing multiple SVM models, we found a C value of 20, with no PCA created our optimum model. This has the best AUC and accuracy with .7 and .638 respectively. This was also our best performing model over all.\n",
    "One benefit of the logistic regression models should be mentioned, and that is that they were much less resource intensive to run. The normal run time for the full logistic regression model was a minute max. The SVM models typically took up to 7 minutes to run the full model. While there is a benefit to an AUC/ accuracy gain here. It is not a model that we would probably want to employ to do real time without adding computing power. But for predictions where time is not an issue, this seems to work best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Weight Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have determined a class balanced logistic regression model is preferred, we wanted to evaluate how much each predictor contributed to the model. We find the coefficient values for weight and then plot them to visually compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the weights\n",
    "# Source: github/jakemdrew/DataMiningNotebook 4\n",
    "\n",
    "weights = lr_balance.coef_.T\n",
    "variable_names = X_train.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Feature Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights to compare them\n",
    "# Source: github/jakemdrew/DataMiningNotebook 4\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = pd.Series(lr_balance.coef_[0],index=X_train.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, odds to win and odds to show are by far the most important predictors. This intuitively makes sense. Let's re-run the logistic regression model again using only those two predictors and compare the performance to the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Logistic Regression to predict win/loss - Class Balance\n",
    "X_train_lite = X_train[['win_odds', 'place_odds']]\n",
    "X_test_lite = X_test[['win_odds', 'place_odds']]\n",
    "\n",
    "\n",
    "lr_lite = LogisticRegression(penalty='l2', C=1.0, class_weight='balanced', solver='liblinear' )\n",
    "lr_lite.fit(X_train_lite,Y_train)\n",
    "yhat = lr_lite.predict(X_test_lite)\n",
    "\n",
    "acc = mt.accuracy_score(Y_test,yhat)\n",
    "auc = roc_auc_score(Y_test_onehot,yhat)\n",
    "conf = mt.confusion_matrix(Y_test,yhat)\n",
    "print(\"Accuracy: \", acc, \"%\" )\n",
    "print(\"AUC: \", auc, \"%\" )\n",
    "print(\"Confusion Matrix: \\n\",conf)\n",
    "\n",
    "weights = lr_lite.coef_.T\n",
    "variable_names = X_train.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the previous model with many predictors, we achieved an accuracy of 55.4863%. For the model where we only used win odds and place odds, we achieved an almost identical accuracy of 55.48001% with a simpler model.\n",
    "\n",
    "Finally, we will look at feature importance for the model with that includes horse_type and horse_country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will not visually plot these weights since the plot becomes too cluttered to read\n",
    "weights = lr_onehot.coef_.T\n",
    "variable_names = X_train_onehot.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tuning our parameters for the logistic regression function, we opted to use the 'L2' penalty. Also know as ridge regression, we can use this penalty to tune data that suffers from multicollinearity. However, the disadvantage of using the 'l2' penalty is that all of our predictors will remain in our final fit, unlike using a lasso or elastic net method which pushes insignificant coefficients to zero. \n",
    "\n",
    "The weights of our features are important in understanding how important a single feature is when predicting whether a particular horse will win or not win a race. Since our response is on a probability scale between 0 and 1 , we have to interpret these weights differently than we would with linear regression. For example, when interpreting the weight of the horse_country_GR categorical feature, we can say that the odds for a win are 1.38 lower if the horse was from Greece, compared to horses from other countries. For a numerical feature like win_odds, an increase in the win_odds, decreases the odds of winning by a factor of .05\n",
    "\n",
    "Based off the weights above, there are particular features of importance. Betting on a horse from Zimbabwe/South Africa increases the odds of that particular horse winning the race by a factor of 1.36. This makes sense as horse racing in Zimbabwe and South Africa is very popular among the rich and famous people there. Not only that, but three of the most famous horse races are in South Africa (https://www.sa-venues.com/activities/horse-racing.php).\n",
    "\n",
    "When looking at the encoded horse type columns, we can see that betting on a colt or a rig will increase your odds more than any other type of horse. These two horses are popular among competitors as they are male horses usually below the age of four.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Interpret Support Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we began to look at the main factors that contributed to our support vector models, we found that expressing them graphically was the easiest way to get a handle on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_importances(coef, names):\n",
    "    imp = np.absolute(coef)\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 18.5, forward=True)\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "features=balanced_final_train.columns.to_list\n",
    "f_importances(svm_clf.coef_[0], balanced_final_train.columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at this data we can see that by far, win_odds contributed the most to predicting a win. This makes sense as win_odds are based on statistical data and expert knowledge to predict the winner of any given race. The next group of important variables seemed to come in two groups, the first being horse_id, specific horses that just one a large number of competitions, and horse_type. We would expect some collinearity between these two groups but, both are rather crucial in predicting a winner in any given competition. The fact that these two were so dominant in prediction weight in these models in interesting as there are a number of other variables one would normally assume to have more impact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we would assume that given large resources or lack of a time parameter, that the SVM model should be used over logistic regression for this particular data set. It would be interesting to note if by pruning some of the one hot encode values, if we could further increase the accuracy and efficiencies of both models without a loss of prediction ability (variance or bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
